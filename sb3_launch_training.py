# launch_training.py - Easy launcher for different training configurations
import argparse
import os
import sys
from pathlib import Path

def quick_training():
    """Quick training - 100k steps, 8 environments"""
    print("üöÄ Quick Training Mode")
    config = {
        'n_envs': 8,
        'total_timesteps': 100_000,
        'run_name': 'quick_training'
    }
    return config

def full_training():
    """Full training - 2M steps, 32 environments"""  
    print("üöÄ Full Training Mode")
    config = {
        'n_envs': 32,
        'total_timesteps': 2_000_000,
        'run_name': 'full_training'
    }
    return config

def debug_training():
    """Debug training - 10k steps, 2 environments"""
    print("üöÄ Debug Training Mode")
    config = {
        'n_envs': 2,
        'total_timesteps': 10_000,
        'run_name': 'debug_training'
    }
    return config

def custom_training(timesteps, envs):
    """Custom training configuration"""
    print("üöÄ Custom Training Mode")
    config = {
        'n_envs': envs,
        'total_timesteps': timesteps,
        'run_name': f'custom_{timesteps//1000}k_{envs}env'
    }
    return config

def update_config_file(config):
    """Update the scaled_training.py config with new values"""
    # Create a custom config file
    config_code = f"""
# Custom configuration generated by launcher
class ScaledTrainingConfig:
    def __init__(self):
        # Environment settings
        self.task_variant = 'stack_3_bricks'
        self.max_episode_steps = 250
        
        # Parallel training settings
        self.n_envs = {config['n_envs']}
        self.total_timesteps = {config['total_timesteps']}
        
        # PPO hyperparameters
        self.learning_rate = 3e-4
        self.n_steps = 2048
        self.batch_size = max(64, {config['n_envs']} * 4)  # Scale batch size
        self.n_epochs = 10
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_range = 0.2
        self.ent_coef = 0.01
        self.vf_coef = 0.5
        
        # Evaluation settings
        self.eval_freq = max(10_000, {config['total_timesteps']} // 100)
        self.n_eval_episodes = 5
        
        # Checkpointing  
        self.checkpoint_freq = max(50_000, {config['total_timesteps']} // 20)
        
        # Device settings
        self.device = 'cpu'
        
        # Logging
        import time
        self.project_name = "stacking_robot_sb3"
        self.run_name = "{config['run_name']}_" + str(int(time.time()))
        
        # Output directories
        from pathlib import Path
        self.output_dir = Path("./training_results")
        self.models_dir = self.output_dir / "models"
        self.logs_dir = self.output_dir / "logs"
        self.videos_dir = self.output_dir / "videos"
"""
    
    # Write to temporary config file
    with open('training_config_temp.py', 'w') as f:
        f.write(config_code)
    
    print(f"üìÑ Generated config: {config}")

def check_prerequisites():
    """Check if all required files exist"""
    required_files = [
        'sb3_stacking_environment.py',
        'sb3_scaled_training.py'
    ]
    
    missing_files = []
    for file in required_files:
        if not Path(file).exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"‚ùå Missing required files: {missing_files}")
        return False
    
    # Check if wandb is set up
    try:
        import wandb
        if not wandb.api.api_key:
            print("‚ö†Ô∏è  WandB not configured. Run: python setup_wandb.py")
            return False
    except ImportError:
        print("‚ùå WandB not installed. Run: pip install wandb")
        return False
    
    return True

def launch_training(mode, **kwargs):
    """Launch training with specified configuration"""
    
    if not check_prerequisites():
        return False
    
    # Get configuration
    if mode == 'quick':
        config = quick_training()
    elif mode == 'full':
        config = full_training() 
    elif mode == 'debug':
        config = debug_training()
    elif mode == 'custom':
        config = custom_training(kwargs['timesteps'], kwargs['envs'])
    else:
        print(f"‚ùå Unknown mode: {mode}")
        return False
    
    print(f"üìä Training Configuration:")
    print(f"   Environments: {config['n_envs']}")
    print(f"   Total steps: {config['total_timesteps']:,}")
    print(f"   Run name: {config['run_name']}")
    print(f"   Expected duration: ~{config['total_timesteps'] // (config['n_envs'] * 1000)} minutes")
    
    # Confirm with user
    response = input(f"\nüöÄ Start training? [y/N]: ")
    if response.lower() != 'y':
        print("Training cancelled.")
        return False
    
    # Update configuration and launch
    update_config_file(config)
    
    # Import and run with updated config
    print(f"\nüéØ Launching training...")
    
    # Modify the scaled_training module to use our config
    import importlib.util
    
    # Load the scaled training module
    spec = importlib.util.spec_from_file_location("sb3_scaled_training", "sb3_scaled_training.py")
    training_module = importlib.util.module_from_spec(spec)
    
    # Replace the config class
    from training_config_temp import ScaledTrainingConfig
    training_module.ScaledTrainingConfig = ScaledTrainingConfig
    
    # Execute the training
    spec.loader.exec_module(training_module)
    training_module.main()
    
    return True

def main():
    parser = argparse.ArgumentParser(description='Launch DGX-1 training with different configurations')
    parser.add_argument('mode', choices=['quick', 'full', 'debug', 'custom'], 
                       help='Training mode')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='Total timesteps for custom mode')
    parser.add_argument('--envs', type=int, default=8,
                       help='Number of environments for custom mode')
    parser.add_argument('--dry-run', action='store_true',
                       help='Show configuration without starting training')
    
    args = parser.parse_args()
    
    print("ü§ñ DGX-1 Training Launcher")
    print("=" * 40)
    
    if args.dry_run:
        print("üîç Dry run mode - showing configuration only")
        if args.mode == 'custom':
            config = custom_training(args.timesteps, args.envs)
        elif args.mode == 'quick':
            config = quick_training()
        elif args.mode == 'full':
            config = full_training()
        elif args.mode == 'debug':
            config = debug_training()
        
        print(f"Configuration: {config}")
        return
    
    # Launch training
    kwargs = {
        'timesteps': args.timesteps,
        'envs': args.envs
    }
    
    success = launch_training(args.mode, **kwargs)
    
    if success:
        print("üéâ Training launched successfully!")
    else:
        print("üí• Failed to launch training")

if __name__ == "__main__":
    main()